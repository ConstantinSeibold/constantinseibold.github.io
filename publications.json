{
    "publications": [
      {
        "title": "Content and Colour Distillation for Learning Image Translations with the Spatial Profile Loss",
        "authors": ["Saquib Sarfraz", 
                    "Constantin Seibold", "Haroon Khalid", 
                    "Rainer Stiefelhagen"],
        "venue": "BMVC",
        "image": "assets/img/thumbnails/spl.png",
        "arxivLink": "https://arxiv.org/pdf/1908.00274",
        "codeLink": "https://github.com/ssarfraz/SPL",
        "paperLink": "",
        "dataLink": "https://github.com/ssarfraz/SPL/tree/master/FCC_dataset",
        "award": "Best Industry Paper",
        "date": "2019",
        "tags": ["Generative Networks", "Image Translation", "Dataset", "Makeup Transfer"],
        "abstract": "Generative adversarial networks has emerged as a defacto standard for image translation problems. To successfully drive such models, one has to rely on additional networks e.g., discriminators and/or perceptual networks. Training these networks with pixel based losses alone are generally not sufficient to learn the target distribution. In this paper, we propose a novel method of computing the loss directly between the source and target images that enable proper distillation of shape/content and colour/style. We show that this is useful in typical image-to-image translations allowing us to successfully drive the generator without relying on additional networks. We demonstrate this on many difficult image translation problems such as image-to-image domain mapping, single image super-resolution and photo realistic makeup transfer. Our extensive evaluation shows the effectiveness of the proposed formulation and its ability to synthesize realistic images. "
      },
      {
        "title": "Self-Guided Multiple Instance Learning for Weakly Supervised Thoracic Disease Classification and Localization in Chest Radiographs",
        "authors": ["Constantin Seibold", "Jens Kleesiek", "Heinz-Peter Schlemmer", "Rainer Stiefelhagen"],
        "venue": "ACCV",
        "image": "assets/img/thumbnails/sgl.png",
        "arxivLink": "https://arxiv.org/abs/2010.00127",
        "codeLink": "https://github.com/ConstantinSeibold/SGL",
        "paperLink": "https://openaccess.thecvf.com/content/ACCV2020/papers/Seibold_Self-Guided_Multiple_Instance_Learning_for_Weakly_Supervised_Thoracic_DiseaseClassification_and_ACCV_2020_paper.pdf",
        "dataLink": "",
        "award": "",
        "date": "2020",
        "tags": ["Image classification", "classification","Chest X-ray", "Loss", "Localization"],
        "abstract": "Due to the high complexity of medical images and the scarcity of trained personnel, most large-scale radiological datasets are lacking fine-grained annotations and are often only described on image-level. These shortcomings hinder the deployment of automated diagnosis systems, which require human-interpretable justification for their decision process. In this paper, we address the problem of weakly supervised identification and localization of abnormalities in chest radiographs in a multiple-instance learning setting. To that end, we introduce a novel loss function for training convolutional neural networks increasing the localization confidence and assisting the overall disease identification. The loss leverages both image-and patch-level predictions to generate auxiliary supervision and enables specific training at patch-level. Rather than forming strictly binary from the predictions as done in previous loss formulations, we create targets in a more customized manner. This way, the loss accounts for possible misclassification of less certain instances. We show that the supervision provided within the proposed learning scheme leads to better performance and more precise predictions on prevalent datasets for multiple-instance learning as well as on the NIH ChestX-Ray14 benchmark for disease recognition than previously used losses."
      },
      {
        "title": "Every annotation counts: Multi-label deep supervision for medical image segmentation",
        "authors": ["Simon Reiß", "Constantin Seibold", "Alexander Freytag", "Erik Rodner", "Rainer Stiefelhagen"],
        "venue": "CVPR",
        "image": "assets/img/thumbnails/everyannotationcounts.png",
        "arxivLink": "https://arxiv.org/abs/2104.13243",
        "codeLink": "",
        "paperLink": "https://openaccess.thecvf.com/content/CVPR2021/papers/Reiss_Every_Annotation_Counts_Multi-Label_Deep_Supervision_for_Medical_Image_Segmentation_CVPR_2021_paper.pdf",
        "dataLink": "",
        "award": "",
        "date": "2021",
        "tags": ["Semantic Segmentation", "Semi-supervised Segmentation","OCT", "Loss"],
        "abstract": "Pixel-wise segmentation is one of the most data and annotation hungry tasks in our field. Providing representative and accurate annotations is often mission-critical especially for challenging medical applications. In this paper, we propose a semi-weakly supervised segmentation algorithm to overcome this barrier. Our approach is based on a new formulation of deep supervision and student-teacher model and allows for easy integration of different supervision signals. In contrast to previous work, we show that care has to be taken how deep supervision is integrated in lower layers and we present multi-label deep supervision as the most important secret ingredient for success. With our novel training regime for segmentation that flexibly makes use of images that are either fully labeled, marked with bounding boxes, just global labels, or not at all, we are able to cut the requirement for expensive labels by 94.22%-narrowing the gap to the best fully supervised baseline to only 5% mean IoU. Our approach is validated by extensive experiments on retinal fluid segmentation and we provide an in-depth analysis of the anticipated effect each annotation type can have in boosting segmentation performance."
      },
      {
        "title": "Prediction of low-keV monochromatic images from polyenergetic CT scans for improved automatic detection of pulmonary embolism",
        "authors": ["Constantin Seibold", "Matthias A Fink", "Charlotte Goos", "Hans-Ulrich Kauczor", "Heinz-Peter Schlemmer", "Rainer Stiefelhagen", "Jens Kleesiek"],
        "venue": "ISBI",
        "image": "assets/img/thumbnails/monoenergetic.png",
        "arxivLink": "https://arxiv.org/abs/2102.01445",
        "codeLink": "",
        "paperLink": "https://ieeexplore.ieee.org/iel7/9433749/9433753/09433966.pdf",
        "dataLink": "",
        "award": "",
        "date": "2021",
        "tags": ["Image Translation", "Pulmonary Embolism","dual energy CT"],
        "abstract": "Detector-based spectral computed tomography is a recent dual-energy CT (DECT) technology that offers the possibility of obtaining spectral information. From this spectral data, different types of images can be derived, amongst others virtual monoenergetic (monoE) images. MonoE images potentially exhibit decreased artifacts, improve contrast, and overall contain lower noise values, making them ideal candidates for better delineation and thus improved diagnostic accuracy of vascular abnormalities.In this paper, we are training convolutional neural networks (CNN) that can emulate the generation of monoE images from conventional single energy CT acquisitions. For this task, we investigate several commonly used image-translation methods. We demonstrate that these methods while creating visually similar outputs, lead to a poorer performance when used for automatic classification of pulmonary embolism (PE …"
      },
      {
        "title": "A reporting and analysis framework for structured evaluation of COVID-19 clinical and imaging data",
        "authors": ["Gabriel Alexander Salg et al."],
        "venue": "npj Digital Medicine",
        "image": "assets/img/thumbnails/platform.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://www.nature.com/articles/s41746-021-00439-y",
        "dataLink": "",
        "award": "",
        "date": "2021",
        "tags": ["COVID-19", "Platform","Framework"],
        "abstract": "The COVID-19 pandemic has worldwide individual and socioeconomic consequences. Chest computed tomography has been found to support diagnostics and disease monitoring. A standardized approach to generate, collect, analyze, and share clinical and imaging information in the highest quality possible is urgently needed. We developed systematic, computer-assisted and context-guided electronic data capture on the FDA-approved mint LesionTM software platform to enable cloud-based data collection and real-time analysis. The acquisition and annotation include radiological findings and radiomics performed directly on primary imaging data together with information from the patient history and clinical data. As proof of concept, anonymized data of 283 patients with either suspected or confirmed SARS-CoV-2 infection from eight European medical centers were aggregated in data analysis dashboards …"
      },
      {
        "title": "Pose2Drone: A Skeleton-Pose-based Framework for Human-Drone Interaction",
        "authors": ["Zdravko Marinov", "Stanka Vasileva", "Qing Wang", "Constantin Seibold", "Jiaming Zhang", "Rainer Stiefelhagen"],
        "venue": "EUSIPCO",
        "image": "assets/img/thumbnails/pose2drone.png",
        "arxivLink": "https://arxiv.org/abs/2105.13204",
        "codeLink": "https://github.com/Zrrr1997/Pose2Drone",
        "paperLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616116",
        "dataLink": "",
        "award": "",
        "date": "2021",
        "tags": ["Drone", "Pose Estimation","ROS"],
        "abstract": "Drones have become a common tool, which is utilized in many tasks such as aerial photography, surveillance, and delivery. However, operating a drone requires more and more interaction with the user. A natural and safe method for Human-Drone Interaction (HDI) is using gestures. In this paper, we introduce an HDI framework building upon skeleton-based pose estimation. Our framework provides the functionality to control the movement of the drone with simple arm gestures and to follow the user while keeping a safe distance. We also propose a monocular distance estimation method, which is entirely based on image features and does not require any additional depth sensors. To perform comprehensive experiments and quantitative analysis, we create a customized testing dataset. The experiments indicate that our HDI framework can achieve an average of 93.5% accuracy in the recognition of 11 common …"
      },
      {
        "title": "Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games",
        "authors": ["Alina Roitberg", "David Schneider", "Aulia Djamal", "Constantin Seibold", "Simon Reiß", "Rainer Stiefelhagen"],
        "venue": "IROS",
        "image": "assets/img/thumbnails/sims4action.png",
        "arxivLink": "https://arxiv.org/abs/2107.05617",
        "codeLink": "https://github.com/aroitberg/sims4action",
        "paperLink": "https://ieeexplore.ieee.org/iel7/9635848/9635849/09636381.pdf",
        "dataLink": "https://github.com/aroitberg/sims4action",
        "award": "",
        "date": "2021",
        "tags": ["Synthetic Data", "Action Recognition", "Domain Adaptation", "Video Games", "Dataset"],
        "abstract": "Recognizing Activities of Daily Living (ADL) is a vital process for intelligent assistive robots, but collecting large annotated datasets requires time-consuming temporal labeling and raises privacy concerns, e.g., if the data is collected in a real household. In this work, we explore the concept of constructing training examples for ADL recognition by playing life simulation video games and introduce the SIMS4ACTION dataset created with the popular commercial game THE SIMS 4. We build SIMS4ACTION by specifically executing actions-of-interest in a 'top-down' manner, while the gaming circumstances allow us to freely switch between environments, camera angles and subject appearances. While ADL recognition on gaming data is interesting from the theoretical perspective, the key challenge arises from transferring it to the real-world applications, such as smart-homes or assistive robotics. To meet this requirement …"
      },
      {
        "title": "CT angiography clot burden score from data mining of structured reports for pulmonary embolism",
        "authors": ["Matthias A. Fink" , "Victoria L. Mayer", "Thomas Schneider", "Constantin Seibold", "Rainer Stiefelhagen", "Jens Kleesiek", "Tim F. Weber", "Hans-Ulrich Kauczor"],
        "venue": "Radiology",
        "image": "assets/img/thumbnails/cbs.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://pubs.rsna.org/doi/full/10.1148/radiol.2021211013",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Pulmonary Embolism", "Structured Reporting", "Reporting", "Qanadli-Score","CBS-Score","Score"],
        "abstract": "To compare the performance of the Qanadli scoring system with a clot burden score mined from structured pulmonary embolism (PE) reports from CT angiography."
      },
      {
        "title": "Flying guide dog: Walkable path discovery for the visually impaired utilizing drones and transformer-based semantic segmentation",
        "authors": ["Haobin Tan","Chang Chen","Xinyu Luo","Jiaming Zhang","Constantin Seibold","Kailun Yang","Rainer Stiefelhagen"],
        "venue": "IEEE ROBIO",
        "image": "assets/img/thumbnails/guidedog.png",
        "arxivLink": "https://arxiv.org/abs/2108.07007",
        "codeLink": "https://github.com/EckoTan0804/flying-guide-dog",
        "paperLink": "https://ieeexplore.ieee.org/abstract/document/9739520/",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Drone", "Navigation", "Visually Impaired"],
        "abstract": "Lacking the ability to sense ambient environments effectively,blind and visually impaired people (BVIP) face difficulty in walking outdoors,especially in urban areas. Therefore, tools for assisting BVIP are of great importance. In this paper, we propose a novel 'flying guide dog' prototype for BVIP assistance using drone and street view semantic segmentation. Based on the walkable areas extracted from the segmentation prediction,the drone can adjust its movement automatically and thus lead the user to walk along the walkable path. By recognizing the color of pedestrian traffic lights, our prototype can help the user to cross a street safely.Furthermore, we introduce a new dataset named Pedestrian and Vehicle Traffic Lights (PVTL), which is dedicated to traffic light recognition. The result of our user study in real-world scenarios shows that our prototype is effective and easy to use, providing new insight into BVIP …"
      },
      {
        "title": "Reference-guided Pseudo-Label Generation for Medical Semantic Segmentation",
        "authors": ["Constantin Seibold", "Simon Reiß", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "AAAI",
        "image": "assets/img/thumbnails/rpg.png",
        "arxivLink": "https://arxiv.org/abs/2112.00735",
        "codeLink": "",
        "paperLink": "https://ojs.aaai.org/index.php/AAAI/article/view/20114/19873",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Semantic Segmentation","Semi-supervised Segmentation","Anatomy", "Anatomy Segmentation","OCT", "Chest X-ray", "Supervision", "Pseudolabels"],
        "abstract": "Producing densely annotated data is a difficult and tedious task for medical imaging applications. To address this problem, we propose a novel approach to generate supervision for semi-supervised semantic segmentation. We argue that visually similar regions between labeled and unlabeled images likely contain the same semantics and therefore should share their label. Following this thought, we use a small number of labeled images as reference material and match pixels in an unlabeled image to the semantic of the best fitting pixel in a reference set. This way, we avoid pitfalls such as confirmation bias, common in purely prediction-based pseudo-labeling. Since our method does not require any architectural changes or accompanying networks, one can easily insert it into existing frameworks. We achieve the same performance as a standard fully supervised model on X-ray anatomy segmentation, albeit using 95% fewer labeled images. Aside from an in-depth analysis of different aspects of our proposed method, we further demonstrate the effectiveness of our reference-guided learning paradigm by comparing our approach against existing methods for retinal fluid segmentation with competitive performance as we improve upon recent work by up to 15% mean IoU."
      },
      {
        "title": "Hierarchical nearest neighbor graph embedding for efficient dimensionality reduction",
        "authors": ["Saquib Sarfraz", "Marios Koulakis", "Constantin Seibold", "Rainer Stiefelhagen"],
        "venue": "CVPR",
        "image": "assets/img/thumbnails/hnne.png",
        "arxivLink": "https://arxiv.org/abs/2203.12997",
        "codeLink": "https://github.com/koulakis/h-nne",
        "paperLink": "http://openaccess.thecvf.com/content/CVPR2022/papers/Sarfraz_Hierarchical_Nearest_Neighbor_Graph_Embedding_for_Efficient_Dimensionality_Reduction_CVPR_2022_paper.pdf",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Dimensionality Reduction", "Visualization", "Data Exploration"],
        "abstract": "Visual recognition inside the vehicle cabin leads to safer driving and more intuitive human-vehicle interaction but such systems face substantial obstacles as they need to capture different granularities of driver behaviour while dealing with highly limited body visibility and changing illumination. Multimodal recognition mitigates a number of such issues: prediction outcomes of different sensors complement each other due to different modality-specific strengths and weaknesses. While several late fusion methods have been considered in previously published frameworks, they constantly feature different architecture backbones and building blocks making it very hard to isolate the role of the chosen late fusion strategy itself.This paper presents an empirical evaluation of different paradigms for decision-level late fusion in video-based driver observation. We compare seven different mechanisms for joining the results of …"
      },
      {
        "title": "Towards Automatic Parsing of Structured Visual Content through the Use of Synthetic Data",
        "authors": ["Lukas Scholch", "Jonas Steinhauser", "Maximilian Beichter", "Constantin Seibold", "Kailun Yang", "Merlin Knäble", "Thorsten Schwarz", "Alexander Mädche", "Rainer Stiefelhagen"],
        "venue": "ICPR",
        "image": "assets/img/thumbnails/structuredcontent.png",
        "arxivLink": "https://arxiv.org/abs/2204.14136",
        "codeLink": "",
        "paperLink": "https://ieeexplore.ieee.org/iel7/9956007/9955631/09956453.pdf",
        "dataLink": "https://bit.ly/3jN1pJJ",
        "award": "",
        "date": "2022",
        "tags": ["Synthetic Data","Structured Data", "Dataset"],
        "abstract": "Structured Visual Content (SVC) such as graphs, flow charts, or the like are used by authors to illustrate various concepts. While such depictions allow the average reader to better understand the contents, images containing SVCs are typically not machine-readable. This, in turn, not only hinders automated knowledge aggregation, but also the perception of displayed information for visually impaired people. In this work, we propose a synthetic dataset, containing SVCs in the form of images as well as ground truths. We show the usage of this dataset by an application that automatically extracts a graph representation from an SVC image. This is done by training a model via common supervised learning methods. As there currently exist no large-scale public datasets for the detailed analysis of SVC, we propose the Synthetic SVC (SSVC) dataset comprising 12,000 images with respective bounding box annotations and detailed graph representations. Our dataset enables the development of strong models for the interpretation of SVCs while skipping the time-consuming dense data annotation.We evaluate our model on both synthetic and manually annotated data and show the transferability of synthetic to real via various metrics, given the presented application. Here, we evaluate that this proof of concept is possible to some extend and lay down a solid baseline for this task. We discuss the limitations of our approach for further improvements. Our utilized metrics can be used as a tool for future comparisons in this domain."
      },
      {
        "title": "Jointly Optimized Deep Neural Networks to Synthesize Monoenergetic Images from Single-Energy CT Angiography for Improving Classification of Pulmonary Embolism",
        "authors": ["Matthias A Fink", "Constantin Seibold", "Hans-Ulrich Kauczor", "Rainer Stiefelhagen", "Jens Kleesiek"],
        "venue": "Diagnostics",
        "image": "assets/img/thumbnails/monodiagnostics.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://www.mdpi.com/2075-4418/12/5/1224",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Synthetic Data","Image Translation", "Pulmonary Embolism", "dual energy CT", "Computer Tomography"],
        "abstract": "Detector-based spectral CT offers the possibility of obtaining spectral information from which discrete acquisitions at different energy levels can be derived, yielding so-called virtual monoenergetic images (VMI). In this study, we aimed to develop a jointly optimized deep-learning framework based on dual-energy CT pulmonary angiography (DE-CTPA) data to generate synthetic monoenergetic images (SMI) for improving automatic pulmonary embolism (PE) detection in single-energy CTPA scans. For this purpose, we used two datasets: our institutional DE-CTPA dataset D1, comprising polyenergetic arterial series and the corresponding VMI at low-energy levels (40 keV) with 7892 image pairs, and a 10% subset of the 2020 RSNA Pulmonary Embolism CT Dataset D2, which consisted of 161,253 polyenergetic images with dichotomous slice-wise annotations (PE/no PE). We trained a fully convolutional encoder-decoder on D1 to generate SMI from single-energy CTPA scans of D2, which were then fed into a ResNet50 network for training of the downstream PE classification task. The quantitative results on the reconstruction ability of our framework revealed high-quality visual SMI predictions with reconstruction results of 0.984 ± 0.002 (structural similarity) and 41.706 ± 0.547 dB (peak signal-to-noise ratio). PE classification resulted in an AUC of 0.84 for our model, which achieved improved performance compared to other naïve approaches with AUCs up to 0.81. Our study stresses the role of using joint optimization strategies for deep-learning algorithms to improve automatic PE detection. The proposed pipeline may prove to be beneficial for …"
      },
      {
        "title": "Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training",
        "authors": ["Constantin Seibold", "Simon Reiß", "Saquib Sarfraz", "Rainer Stiefelhagen", "Jens Kleesiek"],
        "venue": "MICCAI",
        "image": "assets/img/thumbnails/breakingawayfrom.png",
        "arxivLink": "https://arxiv.org/abs/2205.07139",
        "codeLink": "",
        "paperLink": "https://link.springer.com/chapter/10.1007/978-3-031-16443-9_66",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Vision Language Models","Open Set Recognition", "Chest X-ray"],
        "abstract": "When reading images, radiologists generate text reports describing the findings therein. Current state-of-the-art computer-aided diagnosis tools utilize a fixed set of predefined categories automatically extracted from these medical reports for training. This form of supervision limits the potential usage of models as they are unable to pick up on anomalies outside of their predefined set, thus, making it a necessity to retrain the classifier with additional data when faced with novel classes. In contrast, we investigate direct text supervision to break away from this closed set assumption. By doing so, we avoid noisy label extraction via text classifiers and incorporate more contextual information. We employ a contrastive global-local dual-encoder architecture to learn concepts directly from unstructured medical reports while maintaining its ability to perform free form classification. We investigate relevant properties of open set …"
      },
      {
        "title": "Graph-constrained Contrastive Regularization for Semi-weakly Volumetric Segmentation",
        "authors": ["Simon Reiß", "Constantin Seibold", "Alexander Freytag", "Erik Rodner", "Rainer Stiefelhagen"],
        "venue": "ECCV",
        "image": "assets/img/thumbnails/semiweaksupervision.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810396.pdf",
        "dataLink": "",
        "award": "",
        "date": "2022",
        "tags": ["Semantic Segmentation", "Semi-supervised Segmentation", "Semi-weakly-supervised Segmentation"],
        "abstract": "Semantic volume segmentation suffers from the requirement of having voxel-wise annotated ground-truth data, which requires immense effort to obtain. In this work, we investigate how models can be trained from sparsely annotated volumes, i.e. volumes with only individual slices annotated. By formulating the scenario as a semi-weakly supervised problem where only some regions in the volume are annotated, we obtain surprising results: expensive dense volumetric annotations can be replaced by cheap, partially labeled volumes with limited impact on accuracy if the hypothesis space of valid models gets properly constrained during training. With our Contrastive Constrained Regularization (Con2R), we demonstrate that 3D convolutional models can be trained with less than of only two dimensional ground-truth labels and still reach up to accuracy of fully supervised baseline models with dense …"
      },
      {
        "title": "Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding",
        "authors": ["Constantin Seibold", "Simon Reiß", "Saquib Sarfraz", "Matthias A Fink", "Victoria Mayer", "Jan Sellner", "Moon Sung Kim", "Klaus H Maier-Hein", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "BMVC",
        "image": "assets/img/thumbnails/paxray.png",
        "arxivLink": "https://arxiv.org/pdf/2210.03416",
        "codeLink": "",
        "paperLink": "https://bmvc2022.mpi-inf.mpg.de/58/",
        "dataLink": "https://drive.google.com/drive/u/1/folders/1rzlsZ0bfByRMBoywOPWZW08GNgIwCU9P",
        "award": "",
        "date": "2022",
        "tags": ["Synthetic Data","Chest X-ray", "Visual Grounding", "Vision Language Models", "Semantic Segmentation", "CT Projection", "Dataset"],
        "abstract": "In clinical radiology reports, doctors capture important information about the patient's health status. They convey their observations from raw medical imaging data about the inner structures of a patient. As such, formulating reports requires medical experts to possess wide-ranging knowledge about anatomical regions with their normal, healthy appearance as well as the ability to recognize abnormalities. This explicit grasp on both the patient's anatomy and their appearance is missing in current medical image-processing systems as annotations are especially difficult to gather. This renders the models to be narrow experts e.g. for identifying specific diseases. In this work, we recover this missing link by adding human anatomy into the mix and enable the association of content in medical reports to their occurrence in associated imagery (medical phrase grounding). To exploit anatomical structures in this scenario, we present a sophisticated automatic pipeline to gather and integrate human bodily structures from computed tomography datasets, which we incorporate in our PAXRay: A Projected dataset for the segmentation of Anatomical structures in X-Ray data. Our evaluation shows that methods that take advantage of anatomical information benefit heavily in visually grounding radiologists' findings, as our anatomical segmentations allow for up to absolute 50% better grounding results on the OpenI dataset as compared to commonly used region proposals. "
      },
      {
        "title": "Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues",
        "authors": ["Verena Jasmin Hallitschke", "Tobias Schlumberger", "Philipp Kataliakos", "Zdravko Marinov", "Moon Kim", "Lars Heiliger", "Constantin Seibold", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "ISBI",
        "image": "assets/img/thumbnails/nuclearinteractivesegmentation.png",
        "arxivLink": "https://arxiv.org/pdf/2301.09914",
        "codeLink": "https://github.com/verena-hallitschke/pet-ct-annotate",
        "paperLink": "",
        "dataLink": "",
        "award": "Oral Paper",
        "date": "2023",
        "tags": ["PET-CT", "Interactive Segmentation", "Framework", "Nuclear Medicine"],
        "abstract": "Recently, deep learning enabled the accurate segmentation of various diseases in medical imaging. These performances, however, typically demand large amounts of manual voxel annotations. This tedious process for volumetric data becomes more complex when not all required information is available in a single imaging domain as is the case for PET/CT data. We propose a multimodal interactive segmentation framework that mitigates these issues by combining anatomical and physiological cues from PET/CT data. Our framework utilizes the geodesic distance transform to represent the user annotations and we implement a novel ellipsoid-based user simulation scheme during training. We further propose two annotation interfaces and conduct a user study to estimate their usability. We evaluated our model on the in-domain validation dataset and an unseen PET/CT dataset. "
      },
      {
        "title": "Is There a Role of Artificial Intelligence in Preclinical Imaging?",
        "authors": ["Alina Küper", "Paul Blanc-Durand", "Andrei Gafita", "David Kersting", "Wolfgang P Fendler", "Constantin Seibold", "Alexandros Moraitis", "Katharina Lückerath", "Michelle L James", "Robert Seifert"],
        "venue": "Seminars in Nuclear Medicine",
        "image": "assets/img/thumbnails/preclinical.jpg",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://www.sciencedirect.com/science/article/pii/S0001299823000272",
        "dataLink": "",
        "award": "",
        "date": "2023",
        "tags": ["Review", "Nuclear Medicine"],
        "abstract": "This review provides an overview of the current opportunities for integrating artificial intelligence methods into the field of preclinical imaging research in nuclear medicine. The growing demand for imaging agents and therapeutics that are adapted to specific tumor phenotypes can be excellently served by the evolving multiple capabilities of molecular imaging and theranostics. However, the increasing demand for rapid development of novel, specific radioligands with minimal side effects that excel in diagnostic imaging and achieve significant therapeutic effects requires a challenging preclinical pipeline: from target identification through chemical, physical, and biological development to the conduct of clinical trials, coupled with dosimetry and various pre, interim, and post-treatment staging images to create a translational feedback loop for evaluating the efficacy of diagnostic or therapeutic ligands. In virtually all …"
      },
      {
        "title": "Decoupled Semantic Prototypes enable learning from diverse annotation types for semi-weakly segmentation in expert-driven domains",
        "authors": ["Simon Reiß", "Constantin Seibold", "Alexander Freytag", "Erik Rodner", "Rainer Stiefelhagen"],
        "venue": "CVPR",
        "image": "assets/img/thumbnails/prototypes.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://openaccess.thecvf.com/content/CVPR2023/papers/Reiss_Decoupled_Semantic_Prototypes_Enable_Learning_From_Diverse_Annotation_Types_for_CVPR_2023_paper.pdf",
        "dataLink": "",
        "award": "",
        "date": "2023",
        "tags": ["Semantic Segmentation","Semi-weakly-supervised Segmentation", "Microscopy"],
        "abstract": "A vast amount of images and pixel-wise annotations allowed our community to build scalable segmentation solutions for natural domains. However, the transfer to expert-driven domains like microscopy applications or medical healthcare remains difficult as domain experts are a critical factor due to their limited availability for providing pixel-wise annotations. To enable affordable segmentation solutions for such domains, we need training strategies which can simultaneously handle diverse annotation types and are not bound to costly pixel-wise annotations. In this work, we analyze existing training algorithms towards their flexibility for different annotation types and scalability to small annotation regimes. We conduct an extensive evaluation in the challenging domain of organelle segmentation and find that existing semi-and semi-weakly supervised training algorithms are not able to fully exploit diverse annotation types. Driven by our findings, we introduce Decoupled Semantic Prototypes (DSP) as a training method for semantic segmentation which enables learning from annotation types as diverse as image-level-, point-, bounding box-, and pixel-wise annotations and which leads to remarkable accuracy gains over existing solutions for semi-weakly segmentation."
      },
      {
        "title": "Accurate Fine-Grained Segmentation of Human Anatomy in Radiographs via Volumetric Pseudo-Labeling",
        "authors": ["Constantin Seibold", "Alexander Jaus", "Matthias A Fink", "Moon Kim", "Simon Reiß", "Ken Herrmann", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "",
        "image": "assets/img/thumbnails/paxray++.png",
        "arxivLink": "https://arxiv.org/pdf/2306.03934",
        "codeLink": "https://github.com/ConstantinSeibold/ChestXRayAnatomySegmentation/tree/main",
        "paperLink": "",
        "dataLink": "https://drive.google.com/drive/folders/1AEJAaPTxVMx9iofY4J4f2x5gpJqE61I2?usp=sharing",
        "award": "",
        "date": "2023",
        "tags": ["Semantic Segmentation","Instance Segmentation","CT Projection", "Anatomy Segmentation", "Computer Tomography", "Semantic Segmentation", "Dataset", "Feature Extraction"],
        "abstract": "Interpreting chest radiographs (CXR) remains challenging due to the ambiguity of overlapping structures such as the lungs, heart, and bones. To address this issue, we propose a novel method for extracting fine-grained anatomical structures in CXR using pseudo-labeling of three-dimensional computed tomography (CT) scans. "
      },
      {
        "title": "Cellvit: Vision transformers for precise cell segmentation and classification",
        "authors": ["Fabian Hörst", "Moritz Rempe", "Lukas Heine", "Constantin Seibold", "Julius Keyl", "Giulia Baldini", "Selma Ugurel", "Jens Siveke", "Barbara Grünwald", "Jan Egger", "Jens Kleesiek"],
        "venue": "Medical Image Analysis",
        "image": "assets/img/thumbnails/cellvit.png",
        "arxivLink": "https://arxiv.org/pdf/2306.15350",
        "codeLink": "https://github.com/TIO-IKIM/CellViT",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Instance Segmentation","Cell Segmentation", "Vision Transformer", "Histopathology"],
        "abstract": "Nuclei detection and segmentation in hematoxylin and eosin-stained (H&E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently published Segment Anything Model and a ViT-encoder pre-trained on 104 million histological image patches - achieving state-of-the-art nuclei detection and instance segmentation performance on the PanNuke dataset with a mean panoptic quality of 0.51 and an F1-detection score of 0.83."
      },
      {
        "title": "Why does my medical AI look at pictures of birds? Exploring the efficacy of transfer learning across domain boundaries",
        "authors": ["Frederic Jonske", "Moon Kim", "Enrico Nasca", "Janis Evers", "Johannes Haubold", "René Hosch", "Felix Nensa", "Michael Kamp", "Constantin Seibold", "Jan Egger", "Jens Kleesiek"],
        "venue": "Computer Methods and Programs in Biomedicine,",
        "image": "assets/img/thumbnails/birds.png",
        "arxivLink": "https://arxiv.org/pdf/2306.17555",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2025",
        "tags": ["Pretraining", "Computer Tomography", "Transfer Learning"],
        "abstract": "It is an open secret that ImageNet is treated as the panacea of pretraining. Particularly in medical machine learning, models not trained from scratch are often finetuned based on ImageNet-pretrained models. We posit that pretraining on data from the domain of the downstream task should almost always be preferred instead. We leverage RadNet-12M, a dataset containing more than 12 million computed tomography (CT) image slices, to explore the efficacy of self-supervised pretraining on medical and natural images. Our experiments cover intra- and cross-domain transfer scenarios, varying data scales, finetuning vs. linear evaluation, and feature space analysis. We observe that intra-domain transfer compares favorably to cross-domain transfer, achieving comparable or improved performance (0.44% - 2.07% performance increase using RadNet pretraining, depending on the experiment) and demonstrate the existence of a domain boundary-related generalization gap and domain-specific learned features."
      },
      {
        "title": "Valuing Vicinity: Memory attention framework for context-based semantic segmentation in histopathology",
        "authors": ["Oliver Ester", "Fabian Hörst", "Constantin Seibold", "Julius Keyl", "Saskia Ting", "Nikolaos Vasileiadis", "Jessica Schmitz", "Philipp Ivanyi", "Viktor Grünwald", "Jan Hinrich Bräsen", "Jan Egger", "Jens Kleesiek"],
        "venue": "Computerized Medical Imaging and Graphics",
        "image": "assets/img/thumbnails/valuingvacinity.jpg",
        "arxivLink": "https://arxiv.org/abs/2210.11822",
        "codeLink": "",
        "paperLink": "https://www.sciencedirect.com/science/article/pii/S0895611123000563",
        "dataLink": "",
        "award": "",
        "date": "2023",
        "tags": ["Semantic Segmentation","Histopathology"],
        "abstract": "The segmentation of histopathological whole slide images into tumourous and non-tumourous types of tissue is a challenging task that requires the consideration of both local and global spatial contexts to classify tumourous regions precisely. The identification of subtypes of tumour tissue complicates the issue as the sharpness of separation decreases and the pathologist’s reasoning is even more guided by spatial context. However, the identification of detailed tissue types is crucial for providing personalized cancer therapies. Due to the high resolution of whole slide images, existing semantic segmentation methods, restricted to isolated image sections, are incapable of processing context information beyond. To take a step towards better context comprehension, we propose a patch neighbour attention mechanism to query the neighbouring tissue context from a patch embedding memory bank and infuse context …"
      },
      {
        "title": "Towards unifying anatomy segmentation: automated generation of a full-body CT dataset via knowledge aggregation and anatomical guidelines",
        "authors": ["Alexander Jaus*", "Constantin Seibold*", "Kelsey Hermann", "Alexandra Walter", "Kristina Giske", "Johannes Haubold", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "ICIP",
        "image": "assets/img/thumbnails/atlas.png",
        "arxivLink": "https://arxiv.org/pdf/2307.13375",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Semantic Segmentation","Computer Tomography", "Anatomy Segmentation", "Dataset"],
        "abstract": "A vast amount of images and pixel-wise annotations allowed our community to build scalable segmentation solutions for natural domains. However, the transfer to expert-driven domains like microscopy applications or medical healthcare remains difficult as domain experts are a critical factor due to their limited availability for providing pixel-wise annotations. To enable affordable segmentation solutions for such domains, we need training strategies which can simultaneously handle diverse annotation types and are not bound to costly pixel-wise annotations. In this work, we analyze existing training algorithms towards their flexibility for different annotation types and scalability to small annotation regimes. We conduct an extensive evaluation in the challenging domain of organelle segmentation and find that existing semi-and semi-weakly supervised training algorithms are not able to fully exploit diverse annotation types. Driven by our findings, we introduce Decoupled Semantic Prototypes (DSP) as a training method for semantic segmentation which enables learning from annotation types as diverse as image-level-, point-, bounding box-, and pixel-wise annotations and which leads to remarkable accuracy gains over existing solutions for semi-weakly segmentation."
      },
      {
        "title": "MedShapeNet--A Large-Scale Dataset of 3D Medical Shapes for Computer Vision",
        "authors": ["Jianning Li et al."],
        "venue": "Biomedical Engineering/Biomedizinische Technik",
        "image": "assets/img/thumbnails/medshapenet.png",
        "arxivLink": "https://arxiv.org/pdf/2308.16139",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "https://medshapenet.ikim.nrw/",
        "award": "",
        "date": "2025",
        "tags": ["Shapes", "Dataset"],
        "abstract": "We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to facilitate the translation of data-driven vision algorithms to medical applications, and it extends the opportunities to adapt SOTA vision algorithms to solve critical medical problems. Besides, the majority of the medical shapes in MedShapeNet are modeled directly on the imaging data of real patients, and therefore it complements well existing shape benchmarks comprising of computer-aided design (CAD) models. MedShapeNet currently includes more than 100,000 medical shapes, and provides annotations in the form of paired data. It is therefore also a freely available repository of 3D models for extended reality (virtual reality …"
      },
      {
        "title": "On the Impact of Cross-Domain Data on German Language Models",
        "authors": ["Amin Dada", "Aokun Chen", "Cheng Peng", "Kaleb E Smith", "Ahmad Idrissi-Yaghir", "Constantin Seibold", "Jianning Li", "Lars Heiliger", "Christoph M Friedrich", "Daniel Truhn", "Jan Egger", "Jiang Bian", "Jens Kleesiek", "Yonghui Wu"],
        "venue": "EMNLP",
        "image": "assets/img/thumbnails/deberta.png",
        "arxivLink": "https://arxiv.org/pdf/2310.07321",
        "codeLink": "https://huggingface.co/ikim-uk-essen",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2023",
        "tags": ["Transformer", "Natural Language Processing", "NLP"],
        "abstract": "Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to over the previous state-of-the-art. "
      },
      {
        "title": "FootCapture: Towards an AR-based System for 3D Foot Object Acquisition through Photogrammetry",
        "authors": ["Valentin Khan-Blouki", "Franziska Seiz", "Nicolas Walter", "Alexander Jaus", "Zdravko Marinov", "Gijs Luijten", "Jan Egger", "Constantin Seibold", "Dirk Solte", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "MIDL",
        "image": "assets/img/thumbnails/footcapture.png",
        "arxivLink": "https://openreview.net/pdf?id=6EaycEaPoh",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["AR", "Photogrammetry", "Foot"],
        "abstract": "The acquisition of accurate 3D models of feet is crucial in fields such as chronic foot wound monitoring, prosthetics design, and orthopedic surgery. However, obtaining precise models of patients' feet typically relies on manual measurements, which is both costly and prone to error. Addressing this need, we introduce FootCapture, a mobile application designed to facilitate the acquisition of precise photographic measurements. Our solution employs augmented reality to intuitively guide untrained users to capture comprehensive photographic data from the correct positions and angles, suitable to create a high-fidelity 3D model of the patient's foot using photogrammetry. To validate our application's utility, we compared FootCapture with Apple's Guided Capture application in a user study with n=7 participants. The results showed FootCapture’s intuitive use and high robustness marking it as a tool worth considering for medical personnel."
      },
      {
        "title": "CLUE: A Clinical Language Understanding Evaluation for LLMs",
        "authors": ["Amin Dada", "Marie Bauer", "Amanda Butler Contreras", "Osman Alperen Koraş", "Constantin Seibold", "Kaleb E Smith", "Jens Kleesiek"],
        "venue": "",
        "image": "assets/img/thumbnails/clue.pdf.png",
        "arxivLink": "https://arxiv.org/pdf/2404.04067",
        "codeLink": "https://github.com/dadaamin/CLUE",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["LLM", "Natural Language Processing", "NLP", "Medical Understanding"],
        "abstract": "Large Language Models (LLMs) have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs address healthcare-specific challenges, including privacy demands and computational constraints. However, evaluation of these models has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. Additionally, there has been no thorough comparison between biomedical and general-domain LLMs for clinical tasks. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters and four existing tasks designed to test the practical applicability of LLMs in healthcare settings. Our evaluation covers several biomedical and general domain LLMs, providing insights into their clinical performance and applicability. CLUE represents a step towards a standardized approach to evaluating and developing LLMs in healthcare to align future model development with the real-world needs of clinical application. We publish our evaluation and data generation scripts: https://github.com/dadaamin/CLUE "
      },
      {
        "title": "Style Transfer and Pseudo-Label Filtering Improve Transferability in Cell Organelle Segmentation Scenarios",
        "authors": ["Dmitrii Seletkov", "Simon Reiß", "Alexander Freytag", "Constantin Seibold", "Rainer Stiefelhagen"],
        "venue": "ISBI",
        "image": "assets/img/thumbnails/isbi_dimitri.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://ieeexplore.ieee.org/abstract/document/10635796",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Style Transfer", "Cell Segmentation", "Pseudolabels"],
        "abstract": "Our community has experienced a lot of progress in analyzing biomedical images driven by semantic segmentation solutions. However, the insufficient ability to adapt to new data distributions limits their applicability. As an example, we observed that cell organelle segmentation models can easily drop by more than 60% in relative accuracy when applied to differently imaged cell data. While bridging this gap is possible by collecting new annotations for new data, it is highly repetitive, inefficient, and expensive. In this work, we evaluate how unsupervised and weakly supervised domain adaptation techniques can help to close this gap more efficiently. We answer the questions of how well domain adaptation techniques perform in cell organelle segmentation and whether easy-to-obtain image-level information gives specific benefits. Based on our findings, we propose StyleFilter: a simple and effective approach that …"
      },
      {
        "title": "Enhancing Contrastive Training for Semi-Supervised Chest X-Ray Analysis Through Gaussian Mixture Models",
        "authors": ["Phuong Quynh Le", "Jens Kleesiek", "Constantin Seibold"],
        "venue": "ISBI",
        "image": "assets/img/thumbnails/isbi_quynh.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://ieeexplore.ieee.org/abstract/document/10635757",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Contrastive Learning", "Chest X-ray", "Semi-supervised Learning"],
        "abstract": "Acquiring appropriate annotations for computer vision is a time-consuming and costly process, especially for medical imaging (i.e. chest radiographs) owed mostly to the necessity for trained radiologists. One solution to this problem can be seen in semi-supervised algorithms that use large amounts of unlabeled data to improve training convergence become essential. One particular solution, contrastive learning, which is a widely recognized method for efficient representation learning, is popular for learning descriptive representations of large datasets. By introducing a Gaussian mixture model, we introduce a trustworthiness factor for the inclusion of samples during contrastive learning. By using the resulting reliable samples, the model can learn more expressive representations and not succumb to semantic outliers. Our findings indicate that even without extensive pretraining, our contrastive approach improves the learning of representations of semi-supervised models, leading to a supervised performance of 81% AUC at just 20% of the labels."
      },
      {
        "title": "IKIM at MEDIQA-M3G 2024: Multilingual Visual Question-Answering for Dermatology through VLM Fine-tuning and LLM Translations",
        "authors": ["Marie Bauer", "Amin Dada", "Constantin Seibold", "Jens Kleesiek"],
        "venue": "Proceedings of the 6th Clinical Natural Language Processing Workshop",
        "image": "assets/img/thumbnails/clinicalnlp.png",
        "arxivLink": "",
        "codeLink": "",
        "paperLink": "https://aclanthology.org/2024.clinicalnlp-1.44.pdf",
        "dataLink": "",
        "award": "First Place Solution: Spanish, Chinese",
        "date": "2024",
        "tags": ["LLM", "Translation", "Visual Question Answering", "VQA"],
        "abstract": "This paper presents our solution to the MEDIQA-M3G Challenge at NAACL-ClinicalNLP 2024. We participated in all three languages, ranking first in Chinese and Spanish and third in English. Our approach utilizes LLaVA-med, an open-source, medical vision-language model (VLM) for visual question-answering in Chinese, and Mixtral-8x7B-instruct, a Large Language Model (LLM) for a subsequent translation into English and Spanish. In addition to our final method, we experiment with alternative approaches: Training three different models for each language instead of translating the results from one model, using different combinations and numbers of input images, and additional training on publicly available data that was not part of the original challenge training set."
      },
      {
        "title": "Tumor likelihood estimation on MRI prostate data by utilizing k-Space information",
        "authors": ["Moritz Rempe", "Fabian Hörst", "Constantin Seibold", "Boris Hadaschik", "Marco Schlimbach", "Jan Egger", "Kevin Kröninger", "Felix Breuer", "Martin Blaimer", "Jens Kleesiek"],
        "venue": "",
        "image": "assets/img/thumbnails/kspace.pdf.png",
        "arxivLink": "https://arxiv.org/pdf/2407.06165",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["k-space", "MRI"],
        "abstract": "We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of utilizing the k-Space for better prostate cancer likelihood estimation in comparison to just using the magnitudinal information in the image domain, with an AUROC of . Additionally, by using high undersampling rates and a simple principal component analysis (PCA) for coil compression, we reduce the time needed for reconstruction by avoiding the time intensive GRAPPA reconstruction algorithm. By using digital undersampling for our experiments, we show that scanning and reconstruction time could be reduced. Even with an undersampling factor of 16, our approach achieves meaningful results, with an AUROC of , using the PCA coil combination and taking into account the k-Space information. With this study, we were able to show the feasibility of preserving phase and k-Space information, with consistent results. Besides preserving valuable information for further diagnostics, this approach can work without the time intensive ADC and reconstruction calculations, greatly reducing the post processing, as well as potential scanning time, increasing patient comfort and allowing a close to real-time prediction."
      },
      {
        "title": "Autopet III challenge: Incorporating anatomical knowledge into nnUNet for lesion segmentation in PET/CT",
        "authors": ["Hamza Kalisch", "Fabian Hörst", "Ken Herrmann", "Jens Kleesiek", "Constantin Seibold"],
        "venue": "MICCAI - AutoPET III",
        "image": "assets/img/thumbnails/Autopet3.png",
        "arxivLink": "https://arxiv.org/pdf/2409.12155",
        "codeLink": "https://github.com/hakal104/autoPETIII/",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["PET-CT", "Anatomy Segmentation"],
        "abstract": "Lesion segmentation in PET/CT imaging is essential for precise tumor characterization, which supports personalized treatment planning and enhances diagnostic precision in oncology. However, accurate manual segmentation of lesions is time-consuming and prone to inter-observer variability. Given the rising demand and clinical use of PET/CT, automated segmentation methods, particularly deep-learning-based approaches, have become increasingly more relevant. The autoPET III Challenge focuses on advancing automated segmentation of tumor lesions in PET/CT images in a multitracer multicenter setting, addressing the clinical need for quantitative, robust, and generalizable solutions. Building on previous challenges, the third iteration of the autoPET challenge introduces a more diverse dataset featuring two different tracers (FDG and PSMA) from two clinical centers. To this extent, we developed a classifier that identifies the tracer of the given PET/CT based on the Maximum Intensity Projection of the PET scan. We trained two individual nnUNet-ensembles for each tracer where anatomical labels are included as a multi-label task to enhance the model's performance. Our final submission achieves cross-validation Dice scores of 76.90% and 61.33% for the publicly available FDG and PSMA datasets, respectively. The code is available at https://github.com/hakal104/autoPETIII/ "
      },
      {
        "title": "Anatomy-guided Pathology Segmentation",
        "authors": ["Alexander Jaus", "Constantin Seibold", "Simon Reiß", "Lukas Heine", "Anton Schily", "Moon Kim", "Fin Hendrik Bahnsen", "Ken Herrmann", "Rainer Stiefelhagen", "Jens Kleesiek"],
        "venue": "MICCAI",
        "image": "assets/img/thumbnails/miccai_alex.pdf.png",
        "arxivLink": "https://arxiv.org/pdf/2407.05844",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2025",
        "tags": ["Anatomy Segmentation", "Chest X-ray", "PET-CT"],
        "abstract": "Pathological structures in medical images are typically deviations from the expected anatomy of a patient. While clinicians consider this interplay between anatomy and pathology, recent deep learning algorithms specialize in recognizing either one of the two, rarely considering the patient's body from such a joint perspective. In this paper, we develop a generalist segmentation model that combines anatomical and pathological information, aiming to enhance the segmentation accuracy of pathological features. Our Anatomy-Pathology Exchange (APEx) training utilizes a query-based segmentation transformer which decodes a joint feature space into query-representations for human anatomy and interleaves them via a mixing strategy into the pathology-decoder for anatomy-informed pathology predictions. In doing so, we are able to report the best results across the board on FDG-PET-CT and Chest X-Ray pathology segmentation tasks with a margin of up to 3.3% as compared to strong baseline methods. Code and models will be publicly available at github.com/alexanderjaus/APEx."
      },
      {
        "title": "Every Component Counts: Rethinking the Measure of Success for Medical Semantic Segmentation in Multi-Instance Segmentation Tasks",
        "authors": ["Alexander Jaus", "Constantin Seibold", "Simon Reiß", "Zdravko Marinov", "Keyi Li", "Zeling Ye", "Stefan Krieg", "Jens Kleesiek", "Rainer Stiefelhagen"],
        "venue": "AAAI",
        "image": "https://picsum.photos/200/300",
        "arxivLink": "https://arxiv.org/pdf/2410.18684",
        "codeLink": "",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Metrics", "Medical Image Analysis", "Semantic Segmentation"],
        "abstract": "We present Connected-Component~(CC)-Metrics, a novel semantic segmentation evaluation protocol, targeted to align existing semantic segmentation metrics to a multi-instance detection scenario in which each connected component matters. We motivate this setup in the common medical scenario of semantic metastases segmentation in a full-body PET/CT. We show how existing semantic segmentation metrics suffer from a bias towards larger connected components contradicting the clinical assessment of scans in which tumor size and clinical relevance are uncorrelated. To rebalance existing segmentation metrics, we propose to evaluate them on a per-component basis thus giving each tumor the same weight irrespective of its size. To match predictions to ground-truth segments, we employ a proximity-based matching criterion, evaluating common metrics locally at the component of interest. Using this approach, we break free of biases introduced by large metastasis for overlap-based metrics such as Dice or Surface Dice. CC-Metrics also improves distance-based metrics such as Hausdorff Distances which are uninformative for small changes that do not influence the maximum or 95th percentile, and avoids pitfalls introduced by directly combining counting-based metrics with overlap-based metrics as it is done in Panoptic Quality."
      },
      {
        "title": "De-Identification of Medical Imaging Data: A Comprehensive Tool for Ensuring Patient Privacy",
        "authors": ["Moritz Rempe", "Lukas Heine", "Constantin Seibold", "Fabian Hörst", "Jens Kleesiek"],
        "venue": "",
        "image": "https://picsum.photos/200/300",
        "arxivLink": "https://arxiv.org/pdf/2410.12402",
        "codeLink": "https://github.com/code-lukas/medical_image_deidentification",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["De-Identification", "Privacy"],
        "abstract": "Medical data employed in research frequently comprises sensitive patient health information (PHI), which is subject to rigorous legal frameworks such as the General Data Protection Regulation (GDPR) or the Health Insurance Portability and Accountability Act (HIPAA). Consequently, these types of data must be pseudonymized prior to utilisation, which presents a significant challenge for many researchers. Given the vast array of medical data, it is necessary to employ a variety of de-identification techniques. To facilitate the anonymization process for medical imaging data, we have developed an open-source tool that can be used to de-identify DICOM magnetic resonance images, computer tomography images, whole slide images and magnetic resonance twix raw data. Furthermore, the implementation of a neural network enables the removal of text within the images. The proposed tool automates an elaborate anonymization pipeline for multiple types of inputs, reducing the need for additional tools used for de-identification of imaging data. We make our code publicly available at https://github.com/code-lukas/medical_image_deidentification."
      },
      {
        "title": "Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints",
        "authors": ["Jonas Nasimzada", "Jens Kleesiek", "Ken Herrmann", "Alina Roitberg", "Constantin Seibold"],
        "venue": "",
        "image": "assets/img/thumbnails/jonas.png",
        "arxivLink": "https://arxiv.org/abs/2409.16382",
        "codeLink": "https://github.com/JonasNasimzada/LetsPlay4Emotion/tree/main",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Synthetic Data", "Pain Recognition", "Video Analysis"],
        "abstract": "Acquiring appropriate annotations for computer vision is a time-consuming and costly process, especially for medical imaging (i.e. chest radiographs) owed mostly to the necessity for trained radiologists. One solution to this problem can be seen in semi-supervised algorithms that use large amounts of unlabeled data to improve training convergence become essential. One particular solution, contrastive learning, which is a widely recognized method for efficient representation learning, is popular for learning descriptive representations of large datasets. By introducing a Gaussian mixture model, we introduce a trustworthiness factor for the inclusion of samples during contrastive learning. By using the resulting reliable samples, the model can learn more expressive representations and not succumb to semantic outliers. Our findings indicate that even without extensive pretraining, our contrastive approach improves the learning of representations of semi-supervised models, leading to a supervised performance of 81% AUC at just 20% of the labels."
      },
      {
        "title": "Spacewalker: Traversing Representation Spaces for Fast Interactive Exploration and Annotation of Unstructured Data",
        "authors": ["Lukas Heine", "Fabian Hörst", "Jana Fragemann", "Gijs Luijten", "Miriam Balzer", "Jan Egger", "Fin Bahnsen", "Saquib Sarfraz", "Jens Kleesiek", "Constantin Seibold"],
        "venue": "",
        "image": "assets/img/thumbnails/spacewalker.pdf.png",
        "arxivLink": "https://arxiv.org/pdf/2409.16793",
        "codeLink": "https://github.com/code-lukas/Spacewalker",
        "paperLink": "",
        "dataLink": "",
        "award": "",
        "date": "2024",
        "tags": ["Dimensionality Reduction", "Annotation", "Data Exploration"],
        "abstract": "Unstructured data in industries such as healthcare, finance, and manufacturing presents significant challenges for efficient analysis and decision making. Detecting patterns within this data and understanding their impact is critical but complex without the right tools. Traditionally, these tasks relied on the expertise of data analysts or labor-intensive manual reviews. In response, we introduce Spacewalker, an interactive tool designed to explore and annotate data across multiple modalities. Spacewalker allows users to extract data representations and visualize them in low-dimensional spaces, enabling the detection of semantic similarities. Through extensive user studies, we assess Spacewalker's effectiveness in data annotation and integrity verification. Results show that the tool's ability to traverse latent spaces and perform multi-modal queries significantly enhances the user's capacity to quickly identify relevant data. Moreover, Spacewalker allows for annotation speed-ups far superior to conventional methods, making it a promising tool for efficiently navigating unstructured data and improving decision making processes. The code of this work is open-source and can be found at: this https URL"
      }
    ]
  }
  